from .bailBenchEval import modelsOfInterest, getOutputPath, getProcessedOutputPath, ROLLOUT_TYPE
from .data.bailBench import loadBailBench
from .prompts.bailTool import getToolParser, calledBailTool, BAIL_TOOL_TYPE
from .prompts.bailString import hasBailStr, BAIL_STR_TYPE
from .prompts.bailPrompt import BAIL_PROMPT_CONTINUE_FIRST_TYPE, BAIL_PROMPT_BAIL_FIRST_TYPE, getBailPromptStatus
from .utils import runBatched, getCachedFilePath, getCachedFileJson, flatten, unflatten
from collections import defaultdict
import ujson
import numpy as np
import torch
import os
import vllm



def getRefusePr(minos, allOutputs, batchSize, includeRawArr=False):
    prompts = [x['content'] for x in loadBailBench()]
    tokenizer = minos.get_tokenizer()
    def getInputsFunc(promptsAndOutputs):
        prompt, outputs = promptsAndOutputs
        inputs = []
        for output in outputs:
            input = f"<|user|>\n{prompt}\n<|assistant|>\n{output}"
            tokenized = tokenizer.encode(input, return_tensors="pt")[0][:8000] # minos is picky about size
            input = tokenizer.decode(tokenized)
            inputs.append(input) 
        return inputs
    
    def processBatchFunc(inputBatch):
        resultArr = []
        embeddings = minos.embed(inputBatch, use_tqdm=False)
        for embedding in embeddings:
            prNoRefuse, prRefuse = torch.nn.functional.softmax(torch.tensor(embedding.outputs.embedding), dim=-1)
            resultArr.append(prRefuse.item())
        return resultArr
    
    def processOutputFunc(convI, inputs, refusePrs):
        return refusePrs

    refusePrs = runBatched(list(zip(prompts, allOutputs)),
                            getInputs=getInputsFunc,
                            processBatch=processBatchFunc,
                            processOutput=processOutputFunc,
                            batchSize=batchSize)
    refusePr = float(np.mean(np.array(refusePrs).flatten()))
    if includeRawArr:
        return refusePr, refusePrs
    else:
        return refusePr


def percentTrue(iterable):
    return np.mean(np.array(list(iterable)).flatten())

def processData(minos, modelId, inferenceType, evalType, bailType, toolParser, allOutputs, batchSize, includeRawArr=False):
    if bailType == ROLLOUT_TYPE:
        refusePr = getRefusePr(minos, allOutputs, batchSize=batchSize, includeRawArr=includeRawArr)
        if includeRawArr:
            refusePr, rawArr = refusePr
        print(modelId, inferenceType, evalType, bailType, refusePr)
        result = {"refusePr": refusePr}
    elif bailType == BAIL_TOOL_TYPE:
        bailds = []
        for outputs in allOutputs:
            bailds.append([calledBailTool(output, evalType, toolParser) for output in outputs])
        rawArr = bailds
        toolBailPr = np.mean(np.array(flatten(bailds))) # converts to float
        print(modelId, inferenceType, evalType, bailType, toolBailPr)
        result = {"toolBailPr": toolBailPr}
    elif bailType == BAIL_STR_TYPE:
        bailds = []
        for outputs in allOutputs:
            bailds.append([hasBailStr(output, evalType) for output in outputs])
        rawArr = bailds
        strBailPr = np.mean(np.array(flatten(bailds))) # converts to float
        print(modelId, inferenceType, evalType, bailType, strBailPr)
        result = {"strBailPr": strBailPr}
    elif bailType in BAIL_PROMPT_BAIL_FIRST_TYPE:
        bailStatus = list(map(getBailPromptStatus, flatten(allOutputs)))
        percentContinue = percentTrue(map(lambda x: x == "ðŸŸ¢", bailStatus))
        percentShuffle = percentTrue(map(lambda x: x == "ðŸ”„", bailStatus))
        percentUnknown = percentTrue(map(lambda x: x == "?", bailStatus))
        rawArr = unflatten(bailStatus, allOutputs)
        print(modelId, inferenceType, evalType, bailType, percentShuffle, percentUnknown, percentContinue)
        result = {"promptBailFirstBailPr": percentShuffle, "promptBailFirstUnknownPr": percentUnknown, "promptBailFirstContinuePr": percentContinue}
    elif bailType in BAIL_PROMPT_CONTINUE_FIRST_TYPE:
        bailStatus = list(map(getBailPromptStatus, flatten(allOutputs)))
        percentContinue = percentTrue(map(lambda x: x == "ðŸŸ¢", bailStatus))
        percentShuffle = percentTrue(map(lambda x: x == "ðŸ”„", bailStatus))
        percentUnknown = percentTrue(map(lambda x: x == "?", bailStatus))
        rawArr = unflatten(bailStatus, allOutputs)
        print(modelId, inferenceType, evalType, bailType, percentShuffle, percentUnknown, percentContinue)
        result = {"promptContinueFirstBailPr": percentShuffle, "promptContinueFirstUnknownPr": percentUnknown, "promptContinueFirstContinuePr": percentContinue}
    if includeRawArr:
        result["rawArr" + bailType] = rawArr
    return result

global minos
minos = None

def processBailBenchEval(batchSize):
    global minos

    collectedResults = defaultdict(lambda:{})
    for modelId, inferenceType, evalType, bailType in modelsOfInterest:
        print(modelId, inferenceType, evalType, bailType)
        outputPath = getOutputPath(modelId, inferenceType, evalType, bailType)
        processedOutputPath = getProcessedOutputPath(modelId, inferenceType, evalType, bailType)
        if os.path.exists(getCachedFilePath(outputPath)):
            def process():
                global minos
                if minos is None: # only load if needed
                    minos = vllm.LLM("NousResearch/Minos-v1", task="embed")
                toolParser = getToolParser(modelId, inferenceType) if bailType == BAIL_TOOL_TYPE else None
                outputs = getCachedFileJson(outputPath, lambda: None)
                return processData(minos, modelId, inferenceType, evalType, bailType, toolParser, outputs, batchSize=batchSize, includeRawArr=True)
            processedData = getCachedFileJson(processedOutputPath, process)
            # join by bail type
            for k,v in processedData.items():
                collectedResults[(modelId, inferenceType, evalType)][k] = v
                if k == 'refusePr':
                    collectedResults[(modelId, inferenceType, evalType)]['noRefusePr'] = 1.0-v
                if k == 'toolBailPr':
                    collectedResults[(modelId, inferenceType, evalType)]['toolContinuePr'] = 1.0-v
                if k == 'strBailPr':
                    collectedResults[(modelId, inferenceType, evalType)]['strContinuePr'] = 1.0-v
    fullResultsOutputPath = getCachedFilePath("bailBenchEvalResults.json")
    with open(fullResultsOutputPath, "w") as f:
        ujson.dump(dict(collectedResults), f)

# This requires vllm==0.8.5
if __name__ == "__main__":
    batchSize = 10000 # minos is smol so large batch is fine
    processBailBenchEval(batchSize)